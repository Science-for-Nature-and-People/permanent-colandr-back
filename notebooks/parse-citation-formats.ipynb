{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RIS Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parse .RIS files from Scopus or Mendeley, as well as plaintext exports from\n",
    "Web of Science; return as a list of dictionaries, where each citation record\n",
    "is a dictionary whose keys are field names and values are field values.\n",
    "\"\"\"\n",
    "import io\n",
    "import re\n",
    "\n",
    "from dateutil.parser import parse as parse_date\n",
    "\n",
    "\n",
    "TAG_KEY_MAPPING = {\n",
    "    'A1': 'primary_authors',  # special: Lastname, Firstname, Suffix\n",
    "    'A2': 'secondary_authors',  # special: Lastname, Firstname, Suffix\n",
    "    'A3': 'tertiary_authors',  # special: Lastname, Firstname, Suffix\n",
    "    'A4': 'subsidiary_authors',  # special: Lastname, Firstname, Suffix\n",
    "    'AB': 'abstract',\n",
    "    'AD': 'author_address',\n",
    "    'AN': 'accession_number',\n",
    "    'AU': 'authors',  # special\n",
    "    'AV': 'location_in_archives',\n",
    "    'BN': 'isbn',\n",
    "    'BP': 'start_page',\n",
    "    'BT': 'bt',\n",
    "    'C1': 'custom_1',\n",
    "    'C2': 'custom_2',\n",
    "    'C3': 'custom_3',\n",
    "    'C4': 'custom_4',\n",
    "    'C5': 'custom_5',\n",
    "    'C6': 'custom_6',\n",
    "    'C7': 'custom_7',\n",
    "    'C8': 'custom_8',\n",
    "    'CA': 'caption',\n",
    "    'CN': 'call_number',\n",
    "    'CP': 'cp',\n",
    "    'CT': 'title_of_unpublished_ref',\n",
    "    'CY': 'place_published',\n",
    "    'DA': 'date',  # special: YYYY, YYYY/MM, YYYY/MM/DD/, or YYYY/MM/DD/other info\n",
    "    'DB': 'name_of_database',\n",
    "    'DE': 'author_keywords',\n",
    "    'DI': 'doi',\n",
    "    'DO': 'doi',\n",
    "    'DP': 'database_provider',\n",
    "    'DT': 'document_type',\n",
    "    'ED': 'editor',\n",
    "    'EF': 'end_file',  # ignore!\n",
    "    'EM': 'email_address',\n",
    "    'EP': 'end_page',\n",
    "    'ER': 'end_of_reference',  # special: must be empty and last tag of record\n",
    "    'ET': 'edition',\n",
    "    'FN': 'file_name',  # ignore!\n",
    "    'ID': 'reference_id',\n",
    "    'IS': 'issue_number',\n",
    "    'J1': 'journal_name_user_abbr_1',\n",
    "    'J2': 'journal_name_user_abbr_2',\n",
    "    'JA': 'journal_name_abbr',\n",
    "    'JF': 'journal_name',\n",
    "    'JO': 'journal_name',\n",
    "    'KW': 'keywords',  # special\n",
    "    'L1': 'link_to_pdf',\n",
    "    'L2': 'link_to_fulltext',\n",
    "    'L3': 'related_records',\n",
    "    'L4': 'figure',\n",
    "    'LA': 'language',\n",
    "    'LB': 'label',\n",
    "    'LK': 'link_to_website',\n",
    "    'M1': 'number',\n",
    "    'M2': 'miscellaneous_2',\n",
    "    'M3': 'type_of_work',\n",
    "    'N1': 'notes',\n",
    "    'N2': 'abstract',\n",
    "    'NV': 'number_of_volumes',\n",
    "    'OP': 'original_publication',\n",
    "    'PB': 'publisher',\n",
    "    'PD': 'publication_date',\n",
    "    'PP': 'publishing_place',\n",
    "    'PT': 'publication_type',\n",
    "    'PY': 'publication_year',  # special: YYYY\n",
    "    'RI': 'reviewed_item',\n",
    "    'RN': 'research_notes',\n",
    "    'RP': 'reprint_status',  # special: 'IN FILE', 'NOT IN FILE', or 'ON REQUEST (MM/DD/YY)'\n",
    "    'SE': 'section',\n",
    "    'SN': 'issn',\n",
    "    'SO': 'source_name',\n",
    "    'SP': 'start_page',\n",
    "    'ST': 'short_title',\n",
    "    'SU': 'supplement',\n",
    "    'T1': 'primary_title',\n",
    "    'T2': 'secondary_title',  # note: journal_title, if applicable\n",
    "    'T3': 'tertiary_title',\n",
    "    'TA': 'translated_author',\n",
    "    'TC': 'times_cited',\n",
    "    'TI': 'title',\n",
    "    'TT': 'translated_title',\n",
    "    'TY': 'type_of_reference',  # special: must be key in REFERENCE_TYPES and first tag of record\n",
    "    'U1': 'user_defined_1',\n",
    "    'U2': 'user_defined_2',\n",
    "    'U3': 'user_defined_3',\n",
    "    'U4': 'user_defined_4',\n",
    "    'U5': 'user_defined_5',\n",
    "    'UR': 'url',\n",
    "    'UT': 'unique_identifier',\n",
    "    'VL': 'volume',\n",
    "    'VO': 'published_standard_number',\n",
    "    'VR': 'version',  # ignore!\n",
    "    'Y1': 'primary_date',  # special: YYYY/\n",
    "    'Y2': 'access_date',\n",
    "}\n",
    "\n",
    "REFERENCE_TYPES_MAPPING = {\n",
    "    'ABST': 'abstract',\n",
    "    'ADVS': 'audiovisual material',\n",
    "    'AGGR': 'aggregated database',\n",
    "    'ANCIENT': 'ancient text',\n",
    "    'ART': 'art work',\n",
    "    'BILL': 'bill/resolution',\n",
    "    'BLOG': 'blog',\n",
    "    'BOOK': 'book',\n",
    "    'CASE': 'case',\n",
    "    'CHAP': 'book chapter',\n",
    "    'CHART': 'chart',\n",
    "    'CLSWK': 'classical cork',\n",
    "    'COMP': 'computer program',\n",
    "    'CONF': 'conference proceeding',\n",
    "    'CPAPER': 'conference paper',\n",
    "    'CTLG': 'catalog',\n",
    "    'DATA': 'data file',\n",
    "    'DBASE': 'online database',\n",
    "    'DICT': 'dictionary',\n",
    "    'EBOOK': 'electronic book',\n",
    "    'ECHAP': 'electronic book chapter',\n",
    "    'EDBOOK': 'edited book',\n",
    "    'EJOUR': 'electronic article',\n",
    "    'ELEC': 'web page',\n",
    "    'ENCYC': 'encyclopedia',\n",
    "    'EQUA': 'equation',\n",
    "    'FIGURE': 'figure',\n",
    "    'GEN': 'generic',\n",
    "    'GOVDOC': 'government document',\n",
    "    'GRANT': 'grant',\n",
    "    'HEAR': 'hearing',\n",
    "    'ICOMM': 'internet communication',\n",
    "    'INPR': 'in press',\n",
    "    'JFULL': 'journal (full)',\n",
    "    'JOUR': 'journal',\n",
    "    'LEGAL': 'legal rule or regulation',\n",
    "    'MANSCPT': 'manuscript',\n",
    "    'MAP': 'map',\n",
    "    'MGZN': 'magazine article',\n",
    "    'MPCT': 'motion picture',\n",
    "    'MULTI': 'online multimedia',\n",
    "    'MUSIC': 'music score',\n",
    "    'NEWS': 'newspaper',\n",
    "    'PAMP': 'pamphlet',\n",
    "    'PAT': 'patent',\n",
    "    'PCOMM': 'personal communication',\n",
    "    'RPRT': 'report',\n",
    "    'SER': 'serial publication',\n",
    "    'SLIDE': 'slide',\n",
    "    'SOUND': 'sound recording',\n",
    "    'STAND': 'standard',\n",
    "    'STAT': 'statute',\n",
    "    'THES': 'thesis/dissertation',\n",
    "    'UNBILL': 'unenacted bill/resolution',\n",
    "    'UNPB': 'unpublished work',\n",
    "    'VIDEO': 'video recording',\n",
    "}\n",
    "\n",
    "MULTI_TAGS = {'A1', 'A2', 'A3', 'A4', 'AD', 'AU', 'KW', 'N1'}\n",
    "IGNORE_TAGS = {'FN', 'VR', 'EF'}\n",
    "START_TAGS = {'TY', 'PT'}\n",
    "END_TAG = 'ER'\n",
    "\n",
    "# TAG_RE = re.compile(r'^([A-Z][A-Z0-9])(  - | )|^(E[FR])(\\s?$|  - | )')\n",
    "TAGv1_RE = re.compile(r'^(?P<tag>[A-Z][A-Z0-9])(  - )')\n",
    "TAGv2_RE = re.compile(r'^(?P<tag>[A-Z][A-Z0-9])( )|^(?P<endtag>E[FR])(\\s?$)')\n",
    "\n",
    "\n",
    "VALUE_SANITIZERS = {\n",
    "    'DA': lambda x: parse_date(x).strftime('%Y-%m-%d'),\n",
    "    'PY': lambda x: int(x),\n",
    "    'TC': lambda x: int(x),\n",
    "    'TY': lambda x: REFERENCE_TYPES_MAPPING.get(x, x),\n",
    "    'Y1': lambda x: parse_date('-'.join(item if item else '01' for item in x[:-1].split('/'))),\n",
    "    'Y2': lambda x: min(parse_date(val) for val in x.split(' through ')),\n",
    "    }\n",
    "\n",
    "\n",
    "def _add_tag_line(tag, line, start_idx, record):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tag (str)\n",
    "        line (str)\n",
    "        start_idx (int)\n",
    "        record (dict)\n",
    "    \"\"\"\n",
    "    key = TAG_KEY_MAPPING[tag]\n",
    "    value = line[start_idx:].strip()\n",
    "    # try to sanitize value, but don't sweat failure\n",
    "    try:\n",
    "        value = VALUE_SANITIZERS[tag](value)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    except Exception:\n",
    "        print('value sanitization error: key={}, value={}'.format(key, value))\n",
    "    # for multi-value tags, append to a list\n",
    "    if tag in MULTI_TAGS:\n",
    "        try:\n",
    "            record[key].append(value)\n",
    "        except KeyError:\n",
    "            record[key] = [value]\n",
    "    # otherwise, add key:value to record\n",
    "    else:\n",
    "        if key in record:\n",
    "            print('duplicate key error: key={}, value={}'.format(key, value))\n",
    "        record[key] = value\n",
    "\n",
    "\n",
    "def parse_ris_file(path):\n",
    "    with io.open(path, mode='r') as f:\n",
    "\n",
    "        in_record = False\n",
    "        tag_re = None\n",
    "        prev_tag = None\n",
    "        record = {}\n",
    "        records = []\n",
    "\n",
    "        for i, line in enumerate(f):\n",
    "\n",
    "            if not line.strip():\n",
    "                continue\n",
    "\n",
    "            # automatically detect regex needed for this RIS file\n",
    "            if tag_re is None:\n",
    "                tag_re = (TAGv1_RE if TAGv1_RE.match(line)\n",
    "                          else TAGv2_RE if TAGv2_RE.match(line)\n",
    "                          else None)\n",
    "                if tag_re is None:\n",
    "                    raise IOError('file {} is not formatted as expected!'.format(path))\n",
    "\n",
    "            tag_match = tag_re.match(line)\n",
    "            if tag_match:\n",
    "\n",
    "                tag = tag_match.group('tag') or tag_match.group('endtag')\n",
    "\n",
    "                if tag in IGNORE_TAGS:\n",
    "                    prev_tag = tag\n",
    "                    continue\n",
    "\n",
    "                elif tag == END_TAG:\n",
    "                    if in_record is False:\n",
    "                        msg = 'found end tag, but not in a record!\\nline: {} {}'.format(i, line.strip())\n",
    "                        raise IOError(msg)\n",
    "                    records.append(record)\n",
    "                    in_record = False\n",
    "                    record = {}\n",
    "                    prev_tag = tag\n",
    "                    continue\n",
    "\n",
    "                elif tag in START_TAGS:\n",
    "                    if in_record is True:\n",
    "                        msg = 'found start tag, but already in a record!\\nline: {} {}'.format(i, line.strip())\n",
    "                        raise IOError(msg)\n",
    "                    in_record = True\n",
    "                    _add_tag_line(tag, line, tag_match.end(), record)\n",
    "                    prev_tag = tag\n",
    "                    continue\n",
    "\n",
    "                if in_record is False:\n",
    "                    raise IOError('start/end tag mismatch!\\nline: {} {}'.format(i, line.strip()))\n",
    "\n",
    "                if tag in TAG_KEY_MAPPING:\n",
    "                    _add_tag_line(tag, line, tag_match.end(), record)\n",
    "                    prev_tag = tag\n",
    "                    continue\n",
    "                                    \n",
    "                # multi-value tag line happens to start with a tag-compliant string\n",
    "                if prev_tag in MULTI_TAGS:\n",
    "                    _add_tag_line(prev_tag, line, 0, record)\n",
    "                    continue\n",
    "                \n",
    "                # no idea what this is, but might as well save it\n",
    "                print('unknown tag: tag={}, line={} \"{}\"'.format(tag, i, line.strip()))\n",
    "                record[tag] = line[tag_match.end():].strip()\n",
    "                \n",
    "            elif prev_tag in MULTI_TAGS:\n",
    "                _add_tag_line(prev_tag, line, 0, record)\n",
    "                continue\n",
    "                \n",
    "            # single-value tag split across multiple lines, ugh\n",
    "            elif line.startswith('   '):\n",
    "                key = TAG_KEY_MAPPING[prev_tag]\n",
    "                record[key] += ' ' + line.strip()\n",
    "\n",
    "            else:\n",
    "                print('bad line: prev_tag={}, line={} \"{}\"'.format(prev_tag, i, line.strip()))\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from dateutil.parser import parse as parse_date\n",
    "\n",
    "\n",
    "TAG_KEY_MAPPING = {\n",
    "    'A1': 'primary_authors',  # special: Lastname, Firstname, Suffix\n",
    "    'A2': 'secondary_authors',  # special: Lastname, Firstname, Suffix\n",
    "    'A3': 'tertiary_authors',  # special: Lastname, Firstname, Suffix\n",
    "    'A4': 'subsidiary_authors',  # special: Lastname, Firstname, Suffix\n",
    "    'AB': 'abstract',\n",
    "    'AD': 'author_address',\n",
    "    'AN': 'accession_number',\n",
    "    'AU': 'authors',  # special\n",
    "    'AV': 'location_in_archives',\n",
    "    'BN': 'isbn',\n",
    "    'BP': 'start_page',\n",
    "    'BT': 'bt',\n",
    "    'C1': 'custom_1',\n",
    "    'C2': 'custom_2',\n",
    "    'C3': 'custom_3',\n",
    "    'C4': 'custom_4',\n",
    "    'C5': 'custom_5',\n",
    "    'C6': 'custom_6',\n",
    "    'C7': 'custom_7',\n",
    "    'C8': 'custom_8',\n",
    "    'CA': 'caption',\n",
    "    'CN': 'call_number',\n",
    "    'CP': 'cp',\n",
    "    'CT': 'title_of_unpublished_ref',\n",
    "    'CY': 'place_published',\n",
    "    'DA': 'date',  # special: YYYY, YYYY/MM, YYYY/MM/DD/, or YYYY/MM/DD/other info\n",
    "    'DB': 'name_of_database',\n",
    "    'DE': 'author_keywords',\n",
    "    'DI': 'doi',\n",
    "    'DO': 'doi',\n",
    "    'DP': 'database_provider',\n",
    "    'DT': 'document_type',\n",
    "    'ED': 'editor',\n",
    "    'EF': 'end_file',  # ignore!\n",
    "    'EM': 'email_address',\n",
    "    'EP': 'end_page',\n",
    "    'ER': 'end_of_reference',  # special: must be empty and last tag of record\n",
    "    'ET': 'edition',\n",
    "    'FN': 'file_name',  # ignore!\n",
    "    'ID': 'reference_id',\n",
    "    'IS': 'issue_number',\n",
    "    'J1': 'journal_name_user_abbr_1',\n",
    "    'J2': 'journal_name_user_abbr_2',\n",
    "    'JA': 'journal_name_abbr',\n",
    "    'JF': 'journal_name',\n",
    "    'JO': 'journal_name',\n",
    "    'KW': 'keywords',  # special\n",
    "    'L1': 'link_to_pdf',\n",
    "    'L2': 'link_to_fulltext',\n",
    "    'L3': 'related_records',\n",
    "    'L4': 'figure',\n",
    "    'LA': 'language',\n",
    "    'LB': 'label',\n",
    "    'LK': 'link_to_website',\n",
    "    'M1': 'number',\n",
    "    'M2': 'miscellaneous_2',\n",
    "    'M3': 'type_of_work',\n",
    "    'N1': 'notes',\n",
    "    'N2': 'abstract',\n",
    "    'NV': 'number_of_volumes',\n",
    "    'OP': 'original_publication',\n",
    "    'PB': 'publisher',\n",
    "    'PD': 'publication_date',\n",
    "    'PP': 'publishing_place',\n",
    "    'PT': 'publication_type',\n",
    "    'PY': 'publication_year',  # special: YYYY\n",
    "    'RI': 'reviewed_item',\n",
    "    'RN': 'research_notes',\n",
    "    'RP': 'reprint_status',  # special: 'IN FILE', 'NOT IN FILE', or 'ON REQUEST (MM/DD/YY)'\n",
    "    'SE': 'section',\n",
    "    'SN': 'issn',\n",
    "    'SO': 'source_name',\n",
    "    'SP': 'start_page',\n",
    "    'ST': 'short_title',\n",
    "    'SU': 'supplement',\n",
    "    'T1': 'primary_title',\n",
    "    'T2': 'secondary_title',  # note: journal_title, if applicable\n",
    "    'T3': 'tertiary_title',\n",
    "    'TA': 'translated_author',\n",
    "    'TC': 'times_cited',\n",
    "    'TI': 'title',\n",
    "    'TT': 'translated_title',\n",
    "    'TY': 'type_of_reference',  # special: must be key in REFERENCE_TYPES and first tag of record\n",
    "    'U1': 'user_defined_1',\n",
    "    'U2': 'user_defined_2',\n",
    "    'U3': 'user_defined_3',\n",
    "    'U4': 'user_defined_4',\n",
    "    'U5': 'user_defined_5',\n",
    "    'UR': 'url',\n",
    "    'UT': 'unique_identifier',\n",
    "    'VL': 'volume',\n",
    "    'VO': 'published_standard_number',\n",
    "    'VR': 'version',  # ignore!\n",
    "    'Y1': 'primary_date',  # special: YYYY/\n",
    "    'Y2': 'access_date',\n",
    "}\n",
    "\n",
    "REFERENCE_TYPES_MAPPING = {\n",
    "    'ABST': 'abstract',\n",
    "    'ADVS': 'audiovisual material',\n",
    "    'AGGR': 'aggregated database',\n",
    "    'ANCIENT': 'ancient text',\n",
    "    'ART': 'art work',\n",
    "    'BILL': 'bill/resolution',\n",
    "    'BLOG': 'blog',\n",
    "    'BOOK': 'book',\n",
    "    'CASE': 'case',\n",
    "    'CHAP': 'book chapter',\n",
    "    'CHART': 'chart',\n",
    "    'CLSWK': 'classical cork',\n",
    "    'COMP': 'computer program',\n",
    "    'CONF': 'conference proceeding',\n",
    "    'CPAPER': 'conference paper',\n",
    "    'CTLG': 'catalog',\n",
    "    'DATA': 'data file',\n",
    "    'DBASE': 'online database',\n",
    "    'DICT': 'dictionary',\n",
    "    'EBOOK': 'electronic book',\n",
    "    'ECHAP': 'electronic book chapter',\n",
    "    'EDBOOK': 'edited book',\n",
    "    'EJOUR': 'electronic article',\n",
    "    'ELEC': 'web page',\n",
    "    'ENCYC': 'encyclopedia',\n",
    "    'EQUA': 'equation',\n",
    "    'FIGURE': 'figure',\n",
    "    'GEN': 'generic',\n",
    "    'GOVDOC': 'government document',\n",
    "    'GRANT': 'grant',\n",
    "    'HEAR': 'hearing',\n",
    "    'ICOMM': 'internet communication',\n",
    "    'INPR': 'in press',\n",
    "    'JFULL': 'journal (full)',\n",
    "    'JOUR': 'journal',\n",
    "    'LEGAL': 'legal rule or regulation',\n",
    "    'MANSCPT': 'manuscript',\n",
    "    'MAP': 'map',\n",
    "    'MGZN': 'magazine article',\n",
    "    'MPCT': 'motion picture',\n",
    "    'MULTI': 'online multimedia',\n",
    "    'MUSIC': 'music score',\n",
    "    'NEWS': 'newspaper',\n",
    "    'PAMP': 'pamphlet',\n",
    "    'PAT': 'patent',\n",
    "    'PCOMM': 'personal communication',\n",
    "    'RPRT': 'report',\n",
    "    'SER': 'serial publication',\n",
    "    'SLIDE': 'slide',\n",
    "    'SOUND': 'sound recording',\n",
    "    'STAND': 'standard',\n",
    "    'STAT': 'statute',\n",
    "    'THES': 'thesis/dissertation',\n",
    "    'UNBILL': 'unenacted bill/resolution',\n",
    "    'UNPB': 'unpublished work',\n",
    "    'VIDEO': 'video recording',\n",
    "}\n",
    "\n",
    "MULTI_TAGS = {'A1', 'A2', 'A3', 'A4', 'AD', 'AU', 'KW', 'N1'}\n",
    "IGNORE_TAGS = {'FN', 'VR', 'EF'}\n",
    "START_TAGS = {'TY', 'PT'}\n",
    "END_TAG = 'ER'\n",
    "\n",
    "TAGv1_RE = re.compile(r'^(?P<tag>[A-Z][A-Z0-9])(  - )')\n",
    "TAGv2_RE = re.compile(r'^(?P<tag>[A-Z][A-Z0-9])( )|^(?P<endtag>E[FR])(\\s?$)')\n",
    "\n",
    "TAG_VALUE_SANITIZERS = {\n",
    "    'DA': lambda x: parse_date(x).strftime('%Y-%m-%d'),\n",
    "    'PY': lambda x: int(x),\n",
    "    'TC': lambda x: int(x),\n",
    "    'TY': lambda x: REFERENCE_TYPES_MAPPING.get(x, x),\n",
    "    'Y1': lambda x: parse_date('-'.join(item if item else '01' for item in x[:-1].split('/'))),\n",
    "    'Y2': lambda x: min(parse_date(val) for val in x.split(' through ')),\n",
    "    }\n",
    "\n",
    "\n",
    "class RisFile(object):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path (str): RIS file to be parsed\n",
    "        tag_key_mapping (dict): mapping of short RIS tags to human-readable keys\n",
    "        tag_value_sanitizers (dict): mapping of short RIS tags to functions that\n",
    "            sanitize their associated values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path,\n",
    "                 tag_key_mapping=None,\n",
    "                 tag_value_sanitizers=None):\n",
    "        self.path = path\n",
    "        self.tag_key_mapping = (tag_key_mapping if tag_key_mapping\n",
    "                                else TAG_KEY_MAPPING)\n",
    "        self.tag_value_sanitizers = (tag_value_sanitizers if tag_value_sanitizers\n",
    "                                     else TAG_VALUE_SANITIZERS)\n",
    "        self.in_record = False\n",
    "        self.tag_re = None\n",
    "        self.prev_line_len = None\n",
    "        self.prev_tag = None\n",
    "        self.record = {}\n",
    "\n",
    "    def parse(self):\n",
    "        \"\"\"\n",
    "        Yields:\n",
    "            dict: next complete citation record\n",
    "        \n",
    "        Raises:\n",
    "            IOError\n",
    "        \"\"\"\n",
    "        with io.open(self.path, mode='rt') as f:\n",
    "            for i, line in enumerate(f):\n",
    "\n",
    "                # skip empty lines\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "\n",
    "                # automatically detect regex needed for this RIS file\n",
    "                if self.tag_re is None:\n",
    "                    if TAGv1_RE.match(line):\n",
    "                        self.tag_re = TAGv1_RE\n",
    "                    elif TAGv2_RE.match(line):\n",
    "                        self.tag_re = TAGv2_RE\n",
    "                    else:\n",
    "                        msg ='tags in file {} not formatted as expected!'.format(self.path)\n",
    "                        raise IOError(msg)\n",
    "\n",
    "                tag_match = self.tag_re.match(line)\n",
    "                # lines starts with a tag\n",
    "                if tag_match:\n",
    "\n",
    "                    tag = tag_match.group('tag') or tag_match.group('endtag')\n",
    "\n",
    "                    if tag in IGNORE_TAGS:\n",
    "                        self._stash_prev_info(tag, len(line))\n",
    "                        continue\n",
    "\n",
    "                    elif tag == END_TAG:\n",
    "                        if self.in_record is False:\n",
    "                            msg = 'found end tag, but not in a record!\\nline: {} {}'.format(i, line.strip())\n",
    "                            raise IOError(msg)\n",
    "\n",
    "                        yield self.record  # record is complete! spit it out here\n",
    "\n",
    "                        self.in_record = False\n",
    "                        self.record = {}\n",
    "                        self._stash_prev_info(tag, len(line))\n",
    "                        continue\n",
    "\n",
    "                    elif tag in START_TAGS:\n",
    "                        if self.in_record is True:\n",
    "                            msg = 'found start tag, but already in a record!\\nline: {} {}'.format(i, line.strip())\n",
    "                            raise IOError(msg)\n",
    "                        self.in_record = True\n",
    "                        self._add_tag_line(tag, line, tag_match.end())\n",
    "                        self._stash_prev_info(tag, len(line))\n",
    "                        continue\n",
    "\n",
    "                    if self.in_record is False:\n",
    "                        msg = 'start/end tag mismatch!\\nline: {} {}'.format(i, line.strip())\n",
    "                        raise IOError(msg)\n",
    "\n",
    "                    if tag in self.tag_key_mapping:\n",
    "                        self._add_tag_line(tag, line, tag_match.end())\n",
    "                        self._stash_prev_info(tag, len(line))\n",
    "                        continue\n",
    "\n",
    "                    # multi-value tag line happens to start with a tag-compliant string\n",
    "                    if self.prev_tag in MULTI_TAGS:\n",
    "                        self._add_tag_line(self.prev_tag, line, 0)\n",
    "                        continue\n",
    "\n",
    "                    # no idea what this is, but might as well save it\n",
    "                    print('unknown tag: tag={}, line={} \"{}\"'.format(tag, i, line.strip()))\n",
    "                    self.record[tag] = line[tag_match.end():].strip()\n",
    "                    self._stash_prev_info(tag, len(line))\n",
    "                    continue\n",
    "\n",
    "                # subsequent line belonging to a multi-value tag\n",
    "                elif self.prev_tag in MULTI_TAGS:\n",
    "                    self._add_tag_line(self.prev_tag, line, 0)\n",
    "                    continue\n",
    "\n",
    "                # single-value tag split across multiple lines, ugh\n",
    "                elif line.startswith('   ') or self.prev_line_len > 70:\n",
    "                    key = self.tag_key_mapping[self.prev_tag]\n",
    "                    self.record[key] += ' ' + line.strip()\n",
    "\n",
    "                else:\n",
    "                    print('bad line: prev_tag={}, line={} \"{}\"'.format(\n",
    "                        self.prev_tag, i, line.strip()))\n",
    "\n",
    "    def _add_tag_line(self, tag, line, start_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tag (str)\n",
    "            line (str)\n",
    "            start_idx (int)\n",
    "        \"\"\"\n",
    "        key = self.tag_key_mapping[tag]\n",
    "        value = line[start_idx:].strip()\n",
    "        # try to sanitize value, but don't sweat failure\n",
    "        try:\n",
    "            value = TAG_VALUE_SANITIZERS[tag](value)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except Exception:\n",
    "            print('value sanitization error: key={}, value={}'.format(key, value))\n",
    "        # for multi-value tags, append to a list\n",
    "        if tag in MULTI_TAGS:\n",
    "            try:\n",
    "                self.record[key].append(value)\n",
    "            except KeyError:\n",
    "                self.record[key] = [value]\n",
    "        # otherwise, add key:value to record\n",
    "        else:\n",
    "            if key in self.record:\n",
    "                print('duplicate key error: key={}, value={}'.format(key, value))\n",
    "            self.record[key] = value\n",
    "    \n",
    "    def _stash_prev_info(self, tag, line_len):\n",
    "        self.prev_tag = tag\n",
    "        self.prev_line_len = line_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = '../data/raw/citation_formats/scopus_to_ris.ris'\n",
    "# fname = '../data/raw/citation_formats/mendeley_to_ris.ris'\n",
    "# fname = '../data/raw/citation_formats/wos_to_plain_text.txt'\n",
    "records = list(RisFile(fname).parse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown tag: tag=ZB, line=37 \"ZB 0\"\n",
      "unknown tag: tag=Z8, line=38 \"Z8 0\"\n",
      "unknown tag: tag=ZR, line=39 \"ZR 0\"\n",
      "unknown tag: tag=ZS, line=40 \"ZS 0\"\n",
      "unknown tag: tag=Z9, line=42 \"Z9 0\"\n",
      "unknown tag: tag=EI, line=44 \"EI 1873-2119\"\n",
      "unknown tag: tag=ZB, line=85 \"ZB 0\"\n",
      "unknown tag: tag=Z8, line=86 \"Z8 0\"\n",
      "unknown tag: tag=ZR, line=87 \"ZR 0\"\n",
      "unknown tag: tag=ZS, line=88 \"ZS 0\"\n",
      "unknown tag: tag=Z9, line=90 \"Z9 0\"\n",
      "unknown tag: tag=EI, line=92 \"EI 1879-1026\"\n",
      "unknown tag: tag=PM, line=94 \"PM 26971215\"\n",
      "unknown tag: tag=ZB, line=129 \"ZB 0\"\n",
      "unknown tag: tag=Z8, line=130 \"Z8 0\"\n",
      "unknown tag: tag=ZR, line=131 \"ZR 0\"\n",
      "unknown tag: tag=ZS, line=132 \"ZS 0\"\n",
      "unknown tag: tag=Z9, line=134 \"Z9 0\"\n",
      "unknown tag: tag=EI, line=135 \"EI 1095-8630\"\n",
      "unknown tag: tag=PM, line=137 \"PM 27019358\"\n",
      "unknown tag: tag=ZB, line=166 \"ZB 0\"\n",
      "unknown tag: tag=Z8, line=167 \"Z8 0\"\n",
      "unknown tag: tag=ZR, line=168 \"ZR 0\"\n",
      "unknown tag: tag=ZS, line=169 \"ZS 0\"\n",
      "unknown tag: tag=Z9, line=171 \"Z9 0\"\n",
      "unknown tag: tag=EI, line=173 \"EI 1944-3986\"\n",
      "unknown tag: tag=ZB, line=212 \"ZB 0\"\n",
      "unknown tag: tag=Z8, line=213 \"Z8 0\"\n",
      "unknown tag: tag=ZR, line=214 \"ZR 0\"\n",
      "unknown tag: tag=ZS, line=215 \"ZS 0\"\n",
      "unknown tag: tag=Z9, line=217 \"Z9 0\"\n",
      "unknown tag: tag=EI, line=218 \"EI 1879-1298\"\n",
      "unknown tag: tag=PM, line=220 \"PM 27035386\"\n",
      "unknown tag: tag=ZB, line=259 \"ZB 0\"\n",
      "unknown tag: tag=Z8, line=260 \"Z8 0\"\n",
      "unknown tag: tag=ZR, line=261 \"ZR 0\"\n",
      "unknown tag: tag=ZS, line=262 \"ZS 0\"\n",
      "unknown tag: tag=Z9, line=264 \"Z9 0\"\n",
      "unknown tag: tag=EI, line=266 \"EI 1090-2414\"\n",
      "unknown tag: tag=PM, line=268 \"PM 26874984\"\n",
      "unknown tag: tag=ZB, line=309 \"ZB 0\"\n",
      "unknown tag: tag=Z8, line=310 \"Z8 0\"\n",
      "unknown tag: tag=ZR, line=311 \"ZR 0\"\n",
      "unknown tag: tag=ZS, line=312 \"ZS 0\"\n",
      "unknown tag: tag=Z9, line=314 \"Z9 0\"\n",
      "unknown tag: tag=EI, line=315 \"EI 1879-1298\"\n",
      "unknown tag: tag=PM, line=317 \"PM 27003367\"\n",
      "unknown tag: tag=ZB, line=354 \"ZB 0\"\n",
      "unknown tag: tag=Z8, line=355 \"Z8 0\"\n",
      "unknown tag: tag=ZR, line=356 \"ZR 0\"\n",
      "unknown tag: tag=ZS, line=357 \"ZS 0\"\n",
      "unknown tag: tag=Z9, line=359 \"Z9 0\"\n",
      "unknown tag: tag=EI, line=361 \"EI 1549-7801\"\n",
      "unknown tag: tag=PM, line=363 \"PM 25641324\"\n",
      "unknown tag: tag=ZB, line=393 \"ZB 0\"\n",
      "unknown tag: tag=Z8, line=394 \"Z8 0\"\n",
      "unknown tag: tag=ZR, line=395 \"ZR 0\"\n",
      "unknown tag: tag=ZS, line=396 \"ZS 0\"\n",
      "unknown tag: tag=Z9, line=398 \"Z9 0\"\n",
      "unknown tag: tag=ZB, line=440 \"ZB 0\"\n",
      "unknown tag: tag=Z8, line=441 \"Z8 0\"\n",
      "unknown tag: tag=ZR, line=442 \"ZR 0\"\n",
      "unknown tag: tag=ZS, line=443 \"ZS 0\"\n",
      "unknown tag: tag=Z9, line=445 \"Z9 0\"\n",
      "unknown tag: tag=EI, line=447 \"EI 1521-0529\"\n"
     ]
    }
   ],
   "source": [
    "fname = '../data/raw/citation_formats/scopus_to_ris.ris'\n",
    "fname = '../data/raw/citation_formats/mendeley_to_ris.ris'\n",
    "fname = '../data/raw/citation_formats/wos_to_plain_text.txt'\n",
    "records = parse_ris_file(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "## BibTex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import bibtexparser\n",
    "from bibtexparser.bparser import BibTexParser\n",
    "from bibtexparser.customization import (convert_to_unicode,\n",
    "                                        author as sanitize_author_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: rename fields to be consistent with RIS a la tag_key_map???\n",
    "\n",
    "def sanitize_record(record):\n",
    "    record = remove_null_fields(record)\n",
    "    record = convert_to_unicode(record)\n",
    "    record = sanitize_type_field(record)\n",
    "    record = sanitize_keyword_field(record)\n",
    "    record = sanitize_pages_field(record)\n",
    "    record = sanitize_author_field(record)\n",
    "    return record\n",
    "    \n",
    "\n",
    "def remove_null_fields(record):\n",
    "    return {key: value for key, value in record.items()\n",
    "            if value}\n",
    "\n",
    "\n",
    "def sanitize_type_field(record):\n",
    "    if 'type' in record:\n",
    "        record['type'] = record['type'].lower()\n",
    "    return record\n",
    "\n",
    "\n",
    "def sanitize_keyword_field(record, sep=',|;'):\n",
    "    if 'keyword' in record:\n",
    "        record['keyword'] = [i.strip() for i in\n",
    "                             re.split(sep, record['keyword'].replace('\\n', ''))]\n",
    "    if 'author_keywords' in record:\n",
    "        record['author_keywords'] = [i.strip() for i in\n",
    "                                     re.split(sep, record['author_keywords'].replace('\\n', ''))]\n",
    "    return record\n",
    "\n",
    "\n",
    "def sanitize_pages_field(record):\n",
    "    if 'pages' in record:\n",
    "        # hyphen, non-breaking hyphen, en dash, em dash, hyphen-minus, minus sign\n",
    "        separators = ('‐', '‑', '–', '—', '-', '−')\n",
    "        for sep in separators:\n",
    "            if sep in record['pages']:\n",
    "                pages = [i.strip().strip(sep)\n",
    "                         for i in record['pages'].split(sep)\n",
    "                         if i]\n",
    "                if len(pages) > 2:\n",
    "                    print('unusual \"pages\" field value: {}', record['pages'])\n",
    "                record['pages'] = pages[0] + '--' + pages[-1]\n",
    "                break\n",
    "    return record\n",
    "\n",
    "\n",
    "class BibTexFile(object):\n",
    "\n",
    "    def __init__(self, path, sanitizer=None):\n",
    "        self.path = path\n",
    "        if sanitizer is None:\n",
    "            sanitizer = sanitize_record\n",
    "        self.parser = BibTexParser()\n",
    "        self.parser.ignore_nonstandard_types = False\n",
    "        self.parser.homogenize_fields = False\n",
    "        self.parser.customization = sanitizer\n",
    "        \n",
    "    def parse(self):\n",
    "        with io.open(self.path, mode='rt') as f:\n",
    "            parsed_data = bibtexparser.load(f, parser=self.parser)\n",
    "        for record in parsed_data.entries:\n",
    "            yield record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unusual \"pages\" field value: {} 913-928 and 17-19\n",
      "['Wang, S., Peng, X.W., Zhong, L.X., Tan, J.W., Jing, S.S., Cao, X., Chen, W., '\n",
      " 'Sun, R.C., An ultralight, elastic, cost-effective, and highly recyclable '\n",
      " 'superabsorbent from microfibrillated cellulose fibers for oil spillage '\n",
      " 'cleanup (2015) J. Mater. Chem. A, 3, pp. 8772-8781',\n",
      " 'Liao, C.Y., Chiou, J.Y., Lin, J.J., Temperature-dependent oil absorption of '\n",
      " 'poly(oxypropylene)amine-intercalated clays for environmental remediation '\n",
      " '(2015) RSC Adv., 5, pp. 100702-100708',\n",
      " 'Wang, G., Zeng, Z.X., Wu, X.D., Ren, T.H., Han, J., Xue, Q.J., '\n",
      " 'Three-dimensional structured sponge with high oil wettability for the '\n",
      " 'clean-up of oil contaminations and separation of oil-water mixtures (2014) '\n",
      " 'Polym. Chem., 5, pp. 5942-5948',\n",
      " 'Souza, R.S., Porto, P.S.S., Pintor, A.M.A., Ruphuy, G., Costa, M.F., '\n",
      " 'Boaventura, R.A.R., Vilar, V.J.P., New insights on the removal of mineral '\n",
      " 'oil from oil-in-water emulsions using cork by-products: effect of salt and '\n",
      " 'surfactants content (2016) Chem. Eng. J., 285, pp. 709-717',\n",
      " 'Zhu, Y.F., Zheng, Y., Wang, F., Wang, A.Q., Monolithic supermacroporous '\n",
      " 'hydrogel prepared from high internal phase emulsions (HIPEs) for fast '\n",
      " 'removal of Cu2+ and Pb2+ (2016) Chem. Eng. J., 284, pp. 422-430',\n",
      " 'Yin, Y.J., Pan, J.M., Cao, J., Ma, Y., Pan, G.Q., Wu, R.R., Dai, X.H., Yan, '\n",
      " 'Y.S., Rationally designed hybrid molecularly imprinted polymer foam for '\n",
      " 'highly efficient λ-cyhalothrin recognition and uptake via twice imprinting '\n",
      " 'strategy (2016) Chem. Eng. J., 286, pp. 485-496',\n",
      " 'Ge, X., Yang, W., Wang, J.T., Long, D.H., Ling, L.C., Qiao, W.M., Flexible '\n",
      " 'carbon nanofiber sponges for highly efficient and recyclable oil absorption '\n",
      " '(2015) RSC Adv., 5, pp. 70025-70031',\n",
      " 'Wang, Z., Wang, D., Qian, Z.C., Guo, J., Dong, H.X., Zhao, N., Xu, J., '\n",
      " 'Robust superhydrophobic bridged silsesquioxane aerogels with tunable '\n",
      " 'performances and their applications (2015) ACS Appl. Mater. Interfaces, 7, '\n",
      " 'pp. 2016-2024',\n",
      " 'Yun, S., Luo, H.J., Gao, Y.F., Ambient-pressure drying synthesis of large '\n",
      " 'resorcinol-formaldehyde-reinforced silica aerogels with enhanced mechanical '\n",
      " 'strength and superhydrophobicity (2014) J. Mater. Chem. A, 2, pp. '\n",
      " '14542-14549',\n",
      " 'Sai, H.Z., Fu, R., Xing, L., Xiang, J.H., Li, Z.Y., Li, F., Zhang, T., '\n",
      " \"Surface modification of bacterial cellulose aerogels' web-like skeleton for \"\n",
      " 'oil/water separation (2015) ACS Appl. Mater. Interfaces, 7, pp. 7373-7381',\n",
      " 'Ha, H., Shanmuganathan, K., Ellison, C.J., Mechanically stable thermally '\n",
      " 'crosslinked Poly(acrylic acid)/reduced graphene oxide aerogels (2015) ACS '\n",
      " 'Appl. Mater. Interfaces, 7, pp. 6220-6229',\n",
      " 'Yang, Y., Liu, Z.J., Huang, J., Wang, C.Y., Multifunctional, robust sponges '\n",
      " 'by a simple adsorption-combustion method (2015) J. Mater. Chem. A, 3, pp. '\n",
      " '5875-5881',\n",
      " 'Karatum, O., Steiner, S.A., Griffin, J.S., Shi, W., Plata, D.L., Flexible, '\n",
      " 'mechanically durable aerogel composites for oil capture and recovery (2015) '\n",
      " 'ACS Appl. Mater. Interfaces, 8, pp. 215-224',\n",
      " 'Khosravi, M., Azizian, S., Synthesis of a novel highly oleophilic and highly '\n",
      " 'hydrophobic sponge for rapid oil spill cleanup (2015) ACS Appl. Mater. '\n",
      " 'Interfaces, 7, pp. 25326-25333',\n",
      " 'Pan, J.M., Zeng, J., Cao, Q., Gao, H.P., Gen, Y.C., Peng, Y.X., Dai, X.H., '\n",
      " 'Yan, Y.S., Hierarchical macro and mesoporous foams synthesized by HIPEs '\n",
      " 'template and interface grafted route for simultaneous removal of '\n",
      " 'λ-cyhalothrin and copper ions (2016) Chem. Eng. J., 284, pp. 1361-1372',\n",
      " 'Zhang, Y.L., Pan, J.M., Chen, Y., Shi, W.D., Yan, Y.S., Yu, L.B., HIPEs '\n",
      " 'template: towards the synthesis of polymeric catalysts with adjustable '\n",
      " 'porous structure, acid-base strength and wettability for biomass energy '\n",
      " 'conversation (2016) Chem. Eng. J., 283, pp. 956-970',\n",
      " 'Jing, P., Fang, X.H., Yan, J.L., Guo, J., Fang, Y., Ultra-low density porous '\n",
      " 'polystyrene monolith: facile preparation and superior application (2013) J. '\n",
      " 'Mater. Chem. A, 1, pp. 10135-10141',\n",
      " 'Vílchez, A., Rodriguez-Abreu, C., Menner, A., Bismarck, A., Esquena, J., '\n",
      " 'Antagonistic effects between magnetite nanoparticles and hydrophobic '\n",
      " 'surfactant in highly concentrated Pickering emulsions (2014) Langmuir, 30, '\n",
      " 'pp. 5064-5074',\n",
      " 'Zhang, N., Jiang, W., Wang, T.H., Gu, J.J., Zhong, S.T., Zhou, S., Xie, T., '\n",
      " 'Fu, J.J., Facile preparation of magnetic poly(styrene-divinylbenzene) foam '\n",
      " 'and its application as an oil absorbent (2015) Ind. Eng. Chem. Res., 54, pp. '\n",
      " '11033-11039',\n",
      " 'Zhou, S., Jiang, W., Wang, T.H., Lu, Y., Highly hydrophobic, compressible, '\n",
      " 'and magnetic polystyrene/Fe3O4/graphene aerogel composite for oil-water '\n",
      " 'separation (2015) Ind. Eng. Chem. Res., 54, pp. 5460-5467',\n",
      " 'Binks, B.P., Rodrigues, J.A., Inversion of emulsions stabilized solely by '\n",
      " 'ionizable nanoparticles (2005) Angew. Chem. Int. Ed., 44, pp. 441-444',\n",
      " 'Wang, Z.Y., Stein, A., Morphology control of carbon, silica, and '\n",
      " 'carbon/silica nanocomposites: from 3D ordered macro-/mesoporous monoliths to '\n",
      " 'shaped mesoporous particles (2008) Chem. Mater., 20, pp. 1029-1040',\n",
      " 'Barbetta, A., Cameron, N.R., Morphology and surface area of emulsion-derived '\n",
      " '(PolyHIPE) solid foams prepared with oil-phase soluble porogenic solvents: '\n",
      " 'three-component surfactant system (2004) Macromolecules, 37, pp. 3202-3213',\n",
      " 'Lee, M.W., An, S., Latthe, S.S., Lee, C., Hong, S., Yoon, S.S., Electrospun '\n",
      " 'polystyrene nanofiber membrane with superhydrophobicity and '\n",
      " 'superoleophilicity for selective separation of water and low viscous oil '\n",
      " '(2013) ACS Appl. Mater. Interfaces, 5, pp. 10597-10604',\n",
      " 'Zhang, Y.L., Wei, S., Liu, F.J., Du, Y.C., Liu, S., Ji, Y.Y., Yokoi, T., '\n",
      " 'Xiao, F.S., Superhydrophobic nanoporous polymers as efficient adsorbents for '\n",
      " 'organic compounds (2009) Nano Today, 4, pp. 135-142',\n",
      " 'Zhu, Q., Chu, Y., Wang, Z.K., Chen, N., Lin, L., Liu, F.T., Pan, Q.M., '\n",
      " 'Robust superhydrophobic polyurethane sponge as a highly reusable '\n",
      " 'oil-absorption material (2013) J. Mater. Chem. A, 1, pp. 5386-5393',\n",
      " 'Yu, S.Z., Tan, H.Y., Wang, J., Liu, X., Zhou, K.B., High porosity '\n",
      " 'supermacroporous polystyrene materials with excellent oil-water separation '\n",
      " 'and gas permeability properties (2015) ACS Appl. Mater. Interfaces, 7, pp. '\n",
      " '6745-6753',\n",
      " 'Korhonen, J.T., Kettunen, M., Ras, R.H., Ikkala, O., Hydrophobic '\n",
      " 'nanocellulose aerogels as floating, sustainable, reusable, and recyclable '\n",
      " 'oil absorbents (2011) ACS Appl. Mater. Interfaces, 3, pp. 1813-1816']\n"
     ]
    }
   ],
   "source": [
    "fname = '../data/raw/citation_formats/mendeley_to_bibtek.bib'\n",
    "fname = '../data/raw/citation_formats/scopus_to_bibtek.bib'\n",
    "\n",
    "for record in BibTexFile(fname).parse():\n",
    "    #pprint(record)\n",
    "    if 'references' in record:\n",
    "        pprint(record['references'].split('; '))\n",
    "        break\n",
    "\n",
    "# with io.open(fname, mode='r') as bibtex_file:\n",
    "#     parser = BibTexParser()\n",
    "#     parser.customization = sanitize_record\n",
    "#     parser.ignore_nonstandard_types = False\n",
    "#     parser.homogenize_fields = False\n",
    "    \n",
    "#     bib_database = bibtexparser.load(bibtex_file, parser=parser)\n",
    "#     entries = list(bib_database.entries)\n",
    "    \n",
    "# entries[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fname = '../data/raw/all_fields_Combined Search_Results_Final.txt'\n",
    "records = []\n",
    "with io.open(fname, mode='rt', encoding='utf8') as f:\n",
    "    record = {}\n",
    "    for i, line in enumerate(f):\n",
    "        if not line.strip():\n",
    "            if record:\n",
    "                records.append(record)\n",
    "            record = {}\n",
    "        else:\n",
    "            try:\n",
    "                field, value = line.split(':', 1)\n",
    "            except ValueError:\n",
    "                print(i, line)\n",
    "            record[field.strip()] = value.strip()\n",
    "            \n",
    "        if i > 1000:\n",
    "            break\n",
    "            \n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/raw/Combined Search_Results_Top_3.xls')\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['Title'].notnull()]['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
